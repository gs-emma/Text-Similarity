{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d17e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bf8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e154240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1875a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    tokens = {'input_ids': [], 'attention_mask': []}\n",
    "    \n",
    "    for text in texts:\n",
    "        new_tokens = tokenizer.encode_plus(text, max_length=128,\n",
    "                                           truncation=True, padding='max_length',\n",
    "                                           return_tensors='pt')\n",
    "        tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "        tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "    \n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "    \n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e0e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(tokens, embeddings):\n",
    "    attention_mask = tokens['attention_mask']\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "\n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "    mean_pooled = summed / summed_mask\n",
    "    \n",
    "    return mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c85d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data):\n",
    "    sim_arr = []\n",
    "    \n",
    "    text1 = data.text1\n",
    "    text2 = data.text2 \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        tokens = tokenize([text1[i], text2[i]])\n",
    "        \n",
    "        outputs = model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "        \n",
    "        mean_pooled = mean_pool(tokens, embeddings)\n",
    "        mean_pooled = mean_pooled.detach().numpy()\n",
    "        \n",
    "        cos_sim = cosine_similarity([mean_pooled[0]], [mean_pooled[1]])\n",
    "        \n",
    "        sim_arr.append(cos_sim)\n",
    "    \n",
    "    return sim_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c999d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_value = test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7788c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9260255\n",
      "0.7161503\n",
      "0.9502964\n",
      "0.90253365\n",
      "0.9898264\n",
      "0.8316078\n",
      "0.7744622\n",
      "0.83685017\n",
      "0.24968006\n",
      "0.69243103\n",
      "0.716396\n",
      "0.8748588\n",
      "0.9388281\n",
      "0.95143384\n"
     ]
    }
   ],
   "source": [
    "output_ans = []\n",
    "\n",
    "for i in similarity_value:\n",
    "    value = i.flatten()[0]\n",
    "    print(value)\n",
    "    if value >= 0.91:\n",
    "        output_ans.append(1)\n",
    "    else:\n",
    "        output_ans.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8423f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score:  0.7142857142857144\n"
     ]
    }
   ],
   "source": [
    "print('roc_auc_score: ', roc_auc_score(data.ans, output_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c68b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
